{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Single layer Bilayer LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fi64aA0FFxcS",
        "outputId": "faacc394-ca23-4b4d-979f-f8e3412d6637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "# !pip install tensorflow==2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JUValOzcHtEK"
      },
      "source": [
        "#Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab_type": "code",
        "id": "_Jpu8qLEFxcY",
        "outputId": "4c51106f-5837-4ef7-8edc-707e35c24629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVakjZ3oICgx"
      },
      "source": [
        "#Read the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wnK5o4Z1Fxcj",
        "outputId": "ffd3965d-f045-4456-8f52-07933c4c074f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data=pd.read_csv(\"/content/gdrive/My Drive/amazon-fine-food-reviews/Reviews.csv\",nrows=100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGNQKvCaISIn"
      },
      "source": [
        "# Drop Duplicates and NA values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cjul88oOFxcr",
        "colab": {}
      },
      "source": [
        "data.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates\n",
        "data.dropna(axis=0,inplace=True)#dropping na"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2cc790b9-dc0e-45b5-f07c-1b643e876f99",
        "id": "XyWhmuEd1-bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "#data.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 88421 entries, 0 to 99999\n",
            "Data columns (total 10 columns):\n",
            "Id                        88421 non-null int64\n",
            "ProductId                 88421 non-null object\n",
            "UserId                    88421 non-null object\n",
            "ProfileName               88421 non-null object\n",
            "HelpfulnessNumerator      88421 non-null int64\n",
            "HelpfulnessDenominator    88421 non-null int64\n",
            "Score                     88421 non-null int64\n",
            "Time                      88421 non-null int64\n",
            "Summary                   88421 non-null object\n",
            "Text                      88421 non-null object\n",
            "dtypes: int64(5), object(5)\n",
            "memory usage: 7.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r0xLYACiFxdJ"
      },
      "source": [
        "#Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0s6IY-x2FxdL",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XZr-u3OEFxdT",
        "outputId": "95be2e0a-db42-4e78-a0e7-a8bc8f2db39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"html.parser\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                                 #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A2QAeCHWFxdY",
        "colab": {}
      },
      "source": [
        "#call the function\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GsRXocxoFxd-",
        "colab": {}
      },
      "source": [
        "#call the function\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L1zLpnqsFxey",
        "colab": {}
      },
      "source": [
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KT_D2cLiLy77"
      },
      "source": [
        "#Drop empty rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sYK390unFxfA",
        "colab": {}
      },
      "source": [
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vm8Fk2TCL7Sp"
      },
      "source": [
        "#Understanding the distribution of the sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MdF76AHHFxgw",
        "outputId": "59a04672-b204-4388-e66b-1f9027f6fce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in data['cleaned_text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data['cleaned_summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j9MZeJIsptJ+AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZKD5VOWqFxhC",
        "colab": {}
      },
      "source": [
        "max_text_len=100\n",
        "max_summary_len=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yY0tEJP0FxhI",
        "colab": {}
      },
      "source": [
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EwLUH78CFxhg",
        "colab": {}
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RakakKHcFxhl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vq1mqyOHOtIl"
      },
      "source": [
        "\n",
        "#Text Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oRHTgX6hFxhq",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RzvLwYL_PDcx"
      },
      "source": [
        "#Rarewords and its Coverage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8KronV2Fxhx",
        "outputId": "72b561de-d062-4d23-e97d-ba3bc5b5dd2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 64.642614023145\n",
            "Total Coverage of rare words: 1.6609879669070917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "So-J-5kzQIeO"
      },
      "source": [
        "\n",
        "* **tot_cnt** gives the size of vocabulary (which means every unique words in the text)\n",
        " \n",
        "*   **cnt** gives me the no. of rare words whose count falls below threshold\n",
        "\n",
        "*  **tot_cnt - cnt** gives me the top most common words \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J2giEsF3Fxh3",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCbGMsm4FxiA",
        "outputId": "e64ccb19-750f-421e-ed2c-ec314a0cfb30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_voc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15583"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eRHqyBkBFxiJ",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yzE5OiRLFxiM",
        "outputId": "fa25461e-ca7f-4ef0-c6ae-3f2df4f26468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 75.8816749903957\n",
            "Total Coverage of rare words: 3.9278064091959872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-fswLvIgFxiR",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pR8IX9FRFxiY",
        "outputId": "a875fb41-ff62-43df-f3d9-9e9668522647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_tokenizer.word_counts['sostok'],len(y_tr)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73839, 73839)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kZ-vW82sFxih",
        "colab": {}
      },
      "source": [
        "#deleting the rows that contain only START and END tokens\n",
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cx5NISuMFxik",
        "colab": {}
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wOtlDcthFxip"
      },
      "source": [
        "# Model building\n",
        "\n",
        "1 Bilayered LSTM for the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zXef38nBFxir",
        "outputId": "af125395-a51c-4666-d5c0-34a9bf8f845a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "from keras import backend as K \n",
        "from tensorflow.keras.layers import Attention\n",
        "# K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#Bilayered LSTM\n",
        "encoder_lstm1 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c= encoder_lstm1(enc_emb)\n",
        "\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer,\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 100, 100)     1558300     input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_12 (Bidirectional [(None, 100, 600), ( 962400      embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, None, 100)    314000      input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 600)          0           bidirectional_12[0][1]           \n",
            "                                                                 bidirectional_12[0][3]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 600)          0           bidirectional_12[0][2]           \n",
            "                                                                 bidirectional_12[0][4]           \n",
            "__________________________________________________________________________________________________\n",
            "lstm_14 (LSTM)                  [(None, None, 600),  1682400     embedding_8[0][0]                \n",
            "                                                                 concatenate_6[0][0]              \n",
            "                                                                 concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 600),  720600      bidirectional_12[0][0]           \n",
            "                                                                 lstm_14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1200)   0           lstm_14[0][0]                    \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, None, 3140)   3771140     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 9,008,840\n",
            "Trainable params: 9,008,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lwfi1Fm8Fxiz",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s-A3J92MUljB",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ETnPzA4OFxi3",
        "outputId": "c2d633e8-1759-4c6c-f23f-9228f29fa3f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=5,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 72657 samples, validate on 8072 samples\n",
            "Epoch 1/5\n",
            "72657/72657 [==============================] - 428s 6ms/sample - loss: 2.5823 - val_loss: 2.3129\n",
            "Epoch 2/5\n",
            "72657/72657 [==============================] - 423s 6ms/sample - loss: 2.2433 - val_loss: 2.1226\n",
            "Epoch 3/5\n",
            "72657/72657 [==============================] - 419s 6ms/sample - loss: 2.1070 - val_loss: 2.0420\n",
            "Epoch 4/5\n",
            "72657/72657 [==============================] - 419s 6ms/sample - loss: 2.0244 - val_loss: 1.9916\n",
            "Epoch 5/5\n",
            "72657/72657 [==============================] - 419s 6ms/sample - loss: 1.9615 - val_loss: 1.9542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0ezKYOp2UxG5"
      },
      "source": [
        "#Understanding the Diagnostic plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tDTNLAURFxjE",
        "outputId": "9ddb8f0c-fb38-4bb5-acea-47125689480a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKeUmNNCVUspN/D9TOJETm77asAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sBX0zZnOFxjW",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eM_nU_VvFxjq"
      },
      "source": [
        "# Inference\n",
        "\n",
        "Set up the inference for the encoder and decoder according to Bilayered LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9QkrNV-4Fxjt",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim*2))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6f6TTFnBFxj6",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6GuDf4TPWt6_"
      },
      "source": [
        "Functions to convert an integer sequence to a word sequence for summary as well as the reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAUntznIFxj9",
        "colab": {}
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znu7TWZK3maa",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation\n",
        "Evaluating the trained model by calcuating the Rogue Score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZNlkHjQA83U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "totalReviews = len(y_val)\n",
        "listOfReferences = [seq2summary(y_val[i]) for i in range(totalReviews)]\n",
        "listOfHypothesis = [decode_sequence(x_val[i].reshape(1,max_text_len)) for i in range(totalReviews)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft4rrCnoHSC6",
        "colab_type": "code",
        "outputId": "22596824-991e-492d-e1a5-d0e012dae82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "\n",
        "!pip3 install rouge\n",
        "import rouge\n",
        "r = rouge.Rouge()\n",
        "r.get_scores(listOfHypothesis, listOfReferences, avg=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-0.3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.13356852367774855,\n",
              "  'p': 0.1688470432771722,\n",
              "  'r': 0.12322878812906149},\n",
              " 'rouge-2': {'f': 0.025553665800803003,\n",
              "  'p': 0.03248885034687813,\n",
              "  'r': 0.02425256029071688},\n",
              " 'rouge-l': {'f': 0.11966160097097771,\n",
              "  'p': 0.16714568880079292,\n",
              "  'r': 0.12238774639356982}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    }
  ]
}